{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20_NEWSGROUPS IMPLEMENTED AS PER ALGORITHM GIVEN BY TOM MITCHELL IN HIS BOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM**\n",
    " - calculate the size of vocabulary in newsgroups dataset\n",
    " - calculate P(vj) for each class\n",
    " - to calculate P(wk/vj): combine all preprocessed docs of class vj into single doc then count\n",
    " - number of total words and number of times word wk has appeared in vj \n",
    " - you got P(wk/vj) now we need this for all words of test document\n",
    " - multiply the P(wk/vj) with the frequency of occurence class vj, i.e. P(vj)\n",
    " - this gives P(vj/test_example) \n",
    " - similarly, we get the probability for each of 20 classes then print class with highest probability as predicted\n",
    " target class \n",
    "<br>\n",
    " \n",
    "<font color=RED>*THIS IS AN IMPLEMENTATION OF THE NAIVE-BAYES CLASSIFIER WITH LAPLACE CORRECTION WIHTOUT USING MAJOR INBUILT FUNCTIONS FOR PROCESSING OR CLASSIFYING*</font>   \n",
    "<BR>\n",
    "The preprocessing has been done specific to the dataset by evaluating it first on a single news_article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import datasets\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(raw_data):\n",
    "    words = raw_data.split()\n",
    "    text = []\n",
    "    for i in words:\n",
    "        new_str = re.sub('[^ a-zA-Z0-9]', '', i)\n",
    "        text.append(new_str)\n",
    "    # REMOVED ALL PUNCTUATION\n",
    "    words = [str.lower(i) for i in text] \n",
    "    stop = stopwords.words('english')\n",
    "    words_2 = [i for i in words if not i in stop]\n",
    "    # REMOVED STOP WORDS\n",
    "    words = [i for i in words_2 if str.isalpha(i)]\n",
    "    \n",
    "    # NOW WE SHALL REMOVE ALL THOSE WORDS WHOSE FREQUENCY=1\n",
    "    unique, freq = np.unique(words, return_counts=True)\n",
    "    freq_dict = {}\n",
    "    for i in range(len(unique)):\n",
    "        freq_dict[unique[i]] = freq[i]\n",
    "    freq_dict_temp = freq_dict.copy()    \n",
    "    for i in freq_dict_temp:\n",
    "        if(freq_dict_temp[i]<2):        \n",
    "            del freq_dict[i]\n",
    "    final_doc = []\n",
    "    for i in words:\n",
    "        if(i in freq_dict):\n",
    "            final_doc.append(i)\n",
    "    return final_doc\n",
    "# we return each document as a list of words not as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "18846\n",
      "18846\n"
     ]
    }
   ],
   "source": [
    "news = datasets.fetch_20newsgroups(subset='all', shuffle=True)\n",
    "print(news.target_names)\n",
    "print(len(news.data))\n",
    "\n",
    "dataset_x = []\n",
    "for i in news.data:           \n",
    "    document = process(i)\n",
    "    dataset_x.append(document)\n",
    "        \n",
    "# dataset_x contains processed data, news.target contains targets \n",
    "print(len(dataset_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "X_train, X_test, Y_train, Y_test = cv.train_test_split(dataset_x, news.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14134\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_x = []\n",
    "for i in range(20):\n",
    "    corpus_x.append([])\n",
    "    \n",
    "for j in range(len(Y_train)):\n",
    "    corpus_x[Y_train[j]]  = corpus_x[Y_train[j]]+ X_train[j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42382"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_x))\n",
    "len(corpus_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "vocab_ = []\n",
    "for i in corpus_x:\n",
    "   vocab_ = vocab_+i\n",
    "vocabulary = np.unique(vocab_)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words were removed, and any word occurring one time was also removed. The resulting vocabulary contains *33408*  words. now we need to evaluate the formula: \n",
    "\n",
    "<font color=blue>P(vj/wk) = P(wk/vj) * P(vj)</font>\n",
    "<br>\n",
    "<font color=blue>P(wk/vj) = (no_of_times_word_present_class_vj +1)/(no_of_words_in_trainingset_class_vj + vocabulary_size)</font>\n",
    "\n",
    "> We have the *vocabulary_size*. \n",
    "<br>\n",
    "> we need to get a frequency dictionary for each training class in corpus_x\n",
    "<br>\n",
    "> we also need a vector storing the number of words in each class in corpus_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this represents the vector stated above\n",
    "words_class = []\n",
    "for i in corpus_x:\n",
    "    words_class.append(len(i))\n",
    "#words_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - now we create a classwise dictionary of the words in the training set \n",
    "<br>\n",
    "> - where keys = word, value = frequency in the class\n",
    "<br>\n",
    "> - name of this dictionary - glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "glossary = []\n",
    "for i in corpus_x: \n",
    "    unique, freq = np.unique(i, return_counts=True)\n",
    "    freq_dict = {}\n",
    "    for i in range(len(unique)):\n",
    "        freq_dict[unique[i]] = freq[i]\n",
    "    glossary.append(freq_dict)\n",
    "print(type(glossary))\n",
    "print(len(glossary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[611 724 745 725 718 731 714 739 751 739 757 717 749 763 745 745 720 686\n",
      " 578 477]\n",
      "14134\n"
     ]
    }
   ],
   "source": [
    "# we calculate the number of news_articles belonging to each class present in the training data\n",
    "classes, no_of_docs = np.unique(Y_train, return_counts=True)\n",
    "print(no_of_docs)\n",
    "print(sum(no_of_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = X_test[1000]\n",
    "ans = Y_test[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_doc):\n",
    "    max_prob = 0\n",
    "    max_class = 0\n",
    "    v_size = len(vocabulary)\n",
    "    class_proba = no_of_docs/sum(no_of_docs)\n",
    "    for j in range(20):\n",
    "        # testing for jth class\n",
    "        proba = 1\n",
    "        for word in test_doc:\n",
    "            dictn = glossary[j]\n",
    "            if(word in dictn):\n",
    "                term_freq = dictn[word]\n",
    "                doc_freq = words_class[j]\n",
    "                p = (term_freq+1)/(doc_freq+v_size)\n",
    "                proba *= p\n",
    "        proba *= class_proba[j]\n",
    "        if(proba>max_prob):\n",
    "            max_prob = proba\n",
    "            max_class = j\n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "for i in X_test:\n",
    "    ar = predict(i)\n",
    "    Y_pred.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8913412563667233\n"
     ]
    }
   ],
   "source": [
    "# evaluating accuracy\n",
    "count=0\n",
    "for i in range(len(Y_test)):\n",
    "    if(Y_test[i]==Y_pred[i]):\n",
    "        count+=1\n",
    "accuracy = (count/len(X_test))*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HENCE MY CODE LEARNS TO PREDICT THE CORRECT NEWSGROUP AN ARTICLE BELONGS TO OUT OF THE 20 CLASSES WITH 89% ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
